 Q1.Write a PySpark code to read a CSV file named "employees.csv" containing the following columns: "employee_id", "name", "age", "department". Display the top 10 records from the DataFrame.
 ANS:-from pyspark.sql import SparkSession

# Create SparkSession
spark = SparkSession.builder.appName("Read CSV").getOrCreate()

# Read CSV file into a DataFrame
df = spark.read.csv("employees.csv", header=True, inferSchema=True)

# Display the top 10 records
df.show(10)

Q2.  Given a PySpark DataFrame named "sales_data" with columns "product_name" and "revenue", write a code to calculate the total revenue for each product and display the result in descending order
ANS:from pyspark.sql import SparkSession
from pyspark.sql.functions import sum
from pyspark.sql.window import Window

# Create SparkSession
spark = SparkSession.builder.appName("Calculate Total Revenue").getOrCreate()

# Assuming the sales_data DataFrame is already defined

# Calculate total revenue for each product
total_revenue_df = sales_data.groupBy("product_name").agg(sum("revenue").alias("total_revenue"))

# Order by total revenue in descending order
ordered_df = total_revenue_df.orderBy("total_revenue", ascending=False)

# Display the result
ordered_df.show()
Q3:Write a PySpark code to read a JSON file named "students.json" containing student records with the following schema: "name" (string), "age" (integer), "grade" (string). Filter the DataFrame to include only students whose age is greater than 18.
ANS:-from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Create SparkSession
spark = SparkSession.builder.appName("Read JSON").getOrCreate()

# Read JSON file into a DataFrame
df = spark.read.json("students.json")

# Filter the DataFrame to include only students whose age is greater than 18
filtered_df = df.filter(col("age") > 18)

# Display the filtered DataFrame
filtered_df.show()

Q4.Consider a PySpark DataFrame named "transactions" with columns "transaction_id", "user_id", and "amount". Write a code to calculate the average transaction amount for each user and display the result.
ANS:-from pyspark.sql import SparkSession
from pyspark.sql.functions import avg

# Create SparkSession
spark = SparkSession.builder.appName("Calculate Average Transaction Amount").getOrCreate()

# Assuming the transactions DataFrame is already defined

# Calculate average transaction amount for each user
avg_transaction_amount_df = transactions.groupBy("user_id").agg(avg("amount").alias("avg_transaction_amount"))

# Display the result
avg_transaction_amount_df.show()

Q5:- Given a PySpark DataFrame named "logs" with columns "timestamp" (timestamp) and "event" (string), write a code to count the number of events that occurred in each hour and display the result sorted by the hour.
ANS:-from pyspark.sql import SparkSession
from pyspark.sql.functions import hour
from pyspark.sql.window import Window

# Create SparkSession
spark = SparkSession.builder.appName("Count Events by Hour").getOrCreate()

# Assuming the logs DataFrame is already defined

# Extract hour from the timestamp column
logs_with_hour = logs.withColumn("hour", hour("timestamp"))

# Count the number of events per hour
events_per_hour = logs_with_hour.groupBy("hour").count()

# Sort the result by hour
sorted_events = events_per_hour.orderBy("hour")

# Display the result
sorted_events.show()

Q6 .  Retrieve all the customers from the "Customers" table whose age is greater than 25 and have made at least one purchase.
ANS:-from pyspark.sql import SparkSession

# Create SparkSession
spark = SparkSession.builder.appName("Retrieve Customers").getOrCreate()

# Assuming the Customers table is already defined as a DataFrame named "customers"

# Register the DataFrame as a temporary view
customers.createOrReplaceTempView("customers")

# Run SQL query to retrieve customers
query = """
    SELECT *
    FROM customers
    WHERE age > 25 AND purchases > 0
"""

result = spark.sql(query)

# Display the result
result.show()
Q7:-Find the total number of orders placed by each customer and display the results in descending order of the number of orders.
ANS:-from pyspark.sql import SparkSession
from pyspark.sql.functions import count
from pyspark.sql.window import Window

# Create SparkSession
spark = SparkSession.builder.appName("Count Orders").getOrCreate()

# Assuming the orders DataFrame is already defined with customer_id and order_id columns

# Calculate the count of orders per customer
orders_per_customer = orders.groupBy("customer_id").agg(count("order_id").alias("order_count"))

# Order the result by the number of orders in descending order
ordered_result = orders_per_customer.orderBy("order_count", ascending=False)

# Display the result
ordered_result.show()

Q8:-Retrieve the names of all products that are currently out of stock from the "Products" table.
ANS:-from pyspark.sql import SparkSession

# Create SparkSession
spark = SparkSession.builder.appName("Retrieve Out of Stock Products").getOrCreate()

# Assuming the Products table is already defined as a DataFrame named "products"

# Register the DataFrame as a temporary view
products.createOrReplaceTempView("products")

# Run SQL query to retrieve out of stock products
query = """
    SELECT name
    FROM products
    WHERE stock_quantity = 0
"""

result = spark.sql(query)

# Display the result
result.show()

Q9 Calculate the average price of all products in each category and display the results along with the category name.
ANS:-from pyspark.sql import SparkSession
from pyspark.sql.functions import avg

# Create SparkSession
spark = SparkSession.builder.appName("Calculate Average Price").getOrCreate()

# Assuming the products DataFrame is already defined with category and price columns

# Calculate the average price per category
average_price_per_category = products.groupBy("category").agg(avg("price").alias("average_price"))

# Display the result
average_price_per_category.show()

Q10:-Retrieve the top 5 customers who have spent the highest total amount on purchases
ANS:-from pyspark.sql import SparkSession
from pyspark.sql.functions import desc

# Create SparkSession
spark = SparkSession.builder.appName("Top 5 Customers").getOrCreate()

# Assuming the purchases DataFrame is already defined with customer_id and amount columns

# Calculate the total amount spent by each customer
total_amount_per_customer = purchases.groupBy("customer_id").sum("amount").withColumnRenamed("sum(amount)", "total_amount")

# Order the result by total amount spent in descending order
ordered_result = total_amount_per_customer.orderBy(desc("total_amount"))

# Retrieve the top 5 customers
top_5_customers = ordered_result.limit(5)

# Display the result
top_5_customers.show()

















